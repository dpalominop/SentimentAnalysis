{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "tmp = df['review'].str.replace('<[^>]*>|(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', regex=True).replace('[\\W]+', ' ', regex=True)\n",
    "df['review'] = df['review'].str.findall('[\\!|\\¡]').str.join(' ') + \" \" + tmp.str.lower()\n",
    "df.tail(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive-words.txt')\n",
    "positive['words'] = positive['words'].str.replace('<[^>]*>|(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '').replace('[\\W]+', ' ')\n",
    "positive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = pd.read_csv('negative-words.txt', encoding = \"ISO-8859-1\")\n",
    "negative['words'] = negative['words'].str.replace('<[^>]*>|(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '').replace('[\\W]+', ' ')\n",
    "negative.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding=\"utf8\") as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de Nuevas Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# vect = HashingVectorizer(decode_error='ignore', \n",
    "#                          n_features=2**21,\n",
    "#                          preprocessor=None, \n",
    "#                          tokenizer=tokenizer)\n",
    "\n",
    "# Excercise 1: define new features according to https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "def getFeatures(X_test, positive, negative):\n",
    "'''\n",
    "        x1 = 0 # count positive words\n",
    "        x2 = 0 # count negative words\n",
    "        x3 = 0 # 'no' in doc\n",
    "        x4 = 0 # count pronouns\n",
    "        x5 = 0 # '!' in doc\n",
    "        x6 = 0 # log(count of words)\n",
    "        x7 = 1 # Solo usado para obtener el bias\n",
    "'''\n",
    "    \n",
    "    df = pd.DataFrame(data={'review': X_test,\n",
    "                            'x1': np.zeros(len(X_test), dtype=int),\n",
    "                            'x2': np.zeros(len(X_test), dtype=int),\n",
    "                            'x3': np.zeros(len(X_test), dtype=int),\n",
    "                            'x4': np.zeros(len(X_test), dtype=int),\n",
    "                            'x5': np.zeros(len(X_test), dtype=int),\n",
    "                            'x6': np.zeros(len(X_test), dtype=int),\n",
    "                            'x7': np.ones(len(X_test), dtype=int)})\n",
    "    \n",
    "    for i in range(len(positive)):\n",
    "        counter = df['review'].str.count(positive['words'][i])\n",
    "        df['x1'] += counter\n",
    "\n",
    "    for i in range(len(negative)):\n",
    "        counter = df['review'].str.count(negative['words'][i])\n",
    "        df['x2'] += counter\n",
    "\n",
    "    df['x3'] = df['review'].str.contains('no ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('not ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('don ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('dont ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('doesn ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('doesnt ', case=False).astype(int) + \\\n",
    "                df['review'].str.contains('doesnot ', case=False).astype(int)\n",
    "    \n",
    "    df['x4'] = df['review'].str.count('i ') + \\\n",
    "                df['review'].str.count(' me')  + \\\n",
    "                df['review'].str.count('my ')  + \\\n",
    "                df['review'].str.count(' mine') + \\\n",
    "                df['review'].str.count('we ') + \\\n",
    "                df['review'].str.count(' us')  + \\\n",
    "                df['review'].str.count('our')  + \\\n",
    "                df['review'].str.count('ours')  + \\\n",
    "                df['review'].str.count('you ')  + \\\n",
    "                df['review'].str.count('your ')  + \\\n",
    "                df['review'].str.count('yours ')  + \\\n",
    "                df['review'].str.count('myself')  + \\\n",
    "                df['review'].str.count('ourselves')  + \\\n",
    "                df['review'].str.count('yourselves')  + \\\n",
    "                df['review'].str.count(' u ')\n",
    "\n",
    "    df['x5'] = df['review'].str.contains('!', case=False).astype(int)\n",
    "#     df['x5'] = df['review'].str.count('!') + df['review'].str.count('¡')\n",
    "    df['x6'] = np.log(df['review'].str.count(' '))\n",
    "    \n",
    "    df = df.drop(['review'], axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Logistic Regression classifier usando regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import SGDClassifier\n",
    "#clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "# doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "# Excercise 2: implement a Logistic Regression classifier, using regularization, according to https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def stocashticGradientDescent(X,Y,theta,alpha=0.5, lamda=0.2, iterations=10):\n",
    "    m = len(Y)\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        X_i = X.values\n",
    "        Y_i = Y\n",
    "        prediction = sigmoid(np.dot(X_i,theta))\n",
    "        g = np.dot(prediction-Y_i,X_i)\n",
    "        theta = theta - (1/m)*alpha*g + lamda*theta\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a realizar el entrenamiento con 45 mil muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyprind\n",
    "#pbar = pyprind.ProgBar(45)\n",
    "\n",
    "# classes = np.array([0, 1])\n",
    "\n",
    "# for _ in range(45):\n",
    "#     X_train, Y_train = get_minibatch(doc_stream, size=1000)\n",
    "#     X_train = vect.transform(X_train)\n",
    "#     clf.partial_fit(X_train, Y_train, classes=classes)\n",
    "#     #pbar.update()\n",
    "\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "theta = np.zeros(7, dtype=int)\n",
    "\n",
    "%xmode plain\n",
    "%pdb on\n",
    "for i in range(45):\n",
    "    X_train, Y_train = get_minibatch(doc_stream, size=1000)\n",
    "    X_train = getFeatures(X_train, positive, negative, tokenizer)\n",
    "    theta = stocashticGradientDescent(X_train, Y_train, theta, alpha=0.15, lamda=0.2, iterations=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a probar el clasificador usando las 5 mil muestras restantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = getFeatures(X_test, positive, negative, tokenizer)\n",
    "Y_pred = sigmoid(np.dot(X_test,theta))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "accuracy_score(Y_test, (Y_pred>0.5).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the predictive performance, an accuracy of ~87%, is quite \"reasonable\" given that we \"only\" used the default parameters and didn't do any hyperparameter optimization. \n",
    "\n",
    "After we estimated the model perfomance, let us use those last 5,000 test samples to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we successfully trained a model to predict the sentiment of a movie review. Unfortunately, if we'd close this IPython notebook at this point, we'd have to go through the whole learning process again and again if we'd want to make a prediction on \"new data.\"\n",
    "\n",
    "So, to reuse this model, we could use the [`pickle`](https://docs.python.org/3.5/library/pickle.html) module to \"serialize a Python object structure\". Or even better, we could use the [`joblib`](https://pypi.python.org/pypi/joblib) library, which handles large NumPy arrays more efficiently.\n",
    "\n",
    "To install:\n",
    "conda install -c anaconda joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "if not os.path.exists('./pkl_objects'):\n",
    "    os.mkdir('./pkl_objects')\n",
    "    \n",
    "joblib.dump(vect, './vectorizer.pkl')\n",
    "joblib.dump(clf, './clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we \"pickled\" the `HashingVectorizer` and the `SGDClassifier` so that we can re-use those objects later. However, `pickle` and `joblib` have a known issue with `pickling` objects or functions from a `__main__` block and we'd get an `AttributeError: Can't get attribute [x] on <module '__main__'>` if we'd unpickle it later. Thus, to pickle the `tokenizer` function, we can write it to a file and import it to get the `namespace` \"right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "joblib.dump(tokenizer, './tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us restart this IPython notebook and check if the we can load our serialized objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = joblib.load('./tokenizer.pkl')\n",
    "vect = joblib.load('./vectorizer.pkl')\n",
    "clf = joblib.load('./clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `tokenizer`, `HashingVectorizer`, and the tranined logistic regression model, we can use it to make predictions on new data, which can be useful, for example, if we'd want to embed our classifier into a web application -- a topic for another IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['I did not like this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['I loved this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
